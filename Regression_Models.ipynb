{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Simple ML approaches to the Ames Housing regression problem\n","\n","In this notebook I will explore simple regression techniques for the Ames Housing regression competition.\n","This models are simple but, through rigorous application, allow to obtain good results.\n","\n","The notebook focuses on building complete pipelines for ML models:\n","* Preprocessing of data while avoiding information leakage:\n","    * We learn how to combine common feature preprocessing steps such as [1-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics), [standardisation](https://en.wikipedia.org/wiki/Feature_scaling), dimensionality reduction via [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) and [data imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)).\n","    * We use sklearn's [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply different techniques to subsets of the input columns.\n","    * We transform the label using a [quantile transformation](https://stats.stackexchange.com/questions/325570/quantile-transformation-with-gaussian-distribution-sklearn-implementation).\n","* Applying (simple) Machine Learning models:\n","    * A [linear regression model](https://en.wikipedia.org/wiki/Linear_regression)\n","    * A linear regression model, with [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) regularisation.\n","    * An [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) regressor using random trees.\n","    * An [XGBoost](https://en.wikipedia.org/wiki/XGBoost) regressor.\n","    * A [CatBoost](https://en.wikipedia.org/wiki/Catboost) regressor which, different from most other tree-based regressors, can use categorical features natively.\n","    * A [stacked model](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) using all the previous ones."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Ignore some FutureWarning, mainly from Pandas and Sklearn\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.base import RegressorMixin, TransformerMixin\n","from sklearn.compose import ColumnTransformer, TransformedTargetRegressor, make_column_selector\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, QuantileTransformer, FunctionTransformer\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_log_error\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n","\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor"]},{"cell_type":"markdown","metadata":{},"source":["### Reading and basic cleaning of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["categorical_features = [\n","    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n","    'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n","    'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n","    'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n","    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n","    'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n","    'BsmtExposure', 'BsmtFinType1',  'BsmtFinType2', 'Heating',\n","    'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \n","    'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n","    'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n","    'MiscFeature', 'SaleType', 'SaleCondition'\n","]\n","numerical_features = [\n","    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n","    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n","    '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n","    'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n","    'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n","    'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n","    'MiscVal', 'MoSold', 'YrSold'\n","]\n","label = 'SalePrice'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def read_data(filename: str, remove_id: bool = True) -> pd.DataFrame:\n","    \"\"\" Reads a csv file with data and performs basic data cleaning.\n","        It only cleans those columns which can be cleaned without leading\n","        to information leakage. For example, it does not perform imputation.\n","        \n","        Parameters:\n","            filename: path of the csv file\n","            remove_id: if True, it removes the Id column\n","        \n","        Returns:\n","            A pandas dataframe with the processed dataset.\n","    \"\"\"\n","    \n","    d = pd.read_csv(filename)\n","    \n","    # Replace numerical values with meaningful categorical ones for\n","    # column MSSubClass\n","    d.MSSubClass.replace(to_replace={\n","        20: '1StoryNew',\n","        30: '1StoryOld',\n","        40: '1StoryAttic',\n","        45: '1HalfStoryUnfinished',\n","        50: '1HalfStoryFinished',\n","        60: '2StoriesNew',\n","        70: '2StoriesOld',\n","        75: '2HalfStories',\n","        80: 'MultiLevel',\n","        85: 'SplitFoyer',\n","        90: 'Duplex',\n","        120: '1StoryPud',\n","        150: '1HalfStoryPud',\n","        160: '2StoriesPud',\n","        180: 'MultiLevelPud',\n","        190: '2Family'\n","    }, inplace=True)\n","    \n","    # Here I assume that a value of 'NA' for LotFrontage means that the\n","    # house is not directly connected to the street. In this case I can\n","    # replace the value with 0. Another option is that the data is actually\n","    # missing but, given that the rest of the dataset is quite clean,\n","    # I lean towards the first hypothesis.\n","    d.LotFrontage.fillna(value=0, inplace=True)\n","    \n","    # MasVnrArea has NaN instead of 0 in some rows corresponding to houses\n","    # without masonry veener.\n","    d.MasVnrArea.fillna(value=0, inplace=True)\n","    \n","    # Remove column Id which is just noise.\n","    if remove_id and 'Id' in d.columns:\n","        d.drop(columns='Id', inplace=True)\n","    \n","    # Mark categorical features with the appropriate Pandas type.\n","    for feature in categorical_features:\n","        d[feature] = pd.Categorical(d[feature])\n","    \n","    return d"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["d = read_data(filename='data.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### Model selection procedure\n","\n","I use a [holdout validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Holdout_method) method to select among different models and [$k$-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) to perform hyperparameter tuning.\n","\n","Therefore, I first split the data into two parts (houldout method).\n","I will perform hyperparameter tuning on the training (and validation) part, i.e., `X_train` and `y_train`.\n","I will then compare model performance on the test part, i.e., `X_test` and `y_test`."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["features = [column for column in d.columns if column != label]\n","X, y = d[features], d[[label]]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Creating pipelines for our models\n","\n","Function `pipeline_for` below will be the main function I use to create pipelines for our models.\n","It takes an estimator and a number of parameters which define the preprocessing of the data, and combines preprocessing+estimator into a `Pipeline` object.\n","It uses the two functions `categorical_transformations` and `numerical_transformations` which, in turn, build the transformation steps for categorical and numerical features, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def categorical_transformations(\n","    one_hot_encode: bool = True,\n","    one_hot_encode_scale: bool = True,\n","    scaler: TransformerMixin = RobustScaler()\n",") -> Pipeline:\n","    \"\"\" Creates a transformation pipeline for categorical features, to be passed\n","        to a ColumnTransformer. It always performs at least data imputation.\n","        \n","        Parameters:\n","            one_hot_encode: whether to 1-hot encode the features\n","            one_hot_encode_scale: whether to scale the 1-hot encoded features\n","            scaler: the scaler to use, in case scaling should be applied\n","            \n","        Returns:\n","            A pipeline with the preprocessing steps.\n","    \"\"\"\n","\n","    categorical_steps = [\n","        ('imputation', SimpleImputer(strategy='constant', fill_value='None'))\n","    ]\n","    \n","    if one_hot_encode:\n","        # I use handle_unknown='ignore' to make sure no error is raised if, at\n","        # prediction time, the encoder finds a value which it had never seen at\n","        # training time. In this case, using 'ignore' forces the encoder to put\n","        # all 0's in the corresponding 1-hot encoded columns.\n","        categorical_steps.append(\n","            ('one_hot_encoding', OneHotEncoder(handle_unknown='ignore', sparse=False)))\n","\n","        if one_hot_encode_scale:\n","            # Scale the 1-hot encoded features\n","            categorical_steps.append(('one_hot_scaling', scaler))\n","        \n","    return Pipeline(steps=categorical_steps)\n","\n","def numerical_transformations(\n","    numerical_scale: bool = True,\n","    scaler: TransformerMixin = RobustScaler()\n",") -> Pipeline:\n","    \"\"\" Creates a transformation pipeline for numerical features, to be passed\n","        to a ColumnTransformer. It always performs at least data imputation.\n","        \n","        Parameters:\n","            numerical_scale: whether to scale the features\n","            scaler: the scaler to use, in case scaling should be applied\n","            \n","        Returns:\n","            A pipeline with the preprocessing steps.\n","    \"\"\"\n","    \n","    numerical_steps = [('imputation', SimpleImputer(strategy='median'))]\n","\n","    if numerical_scale:\n","        numerical_steps.append(('scaling', scaler))\n","    \n","    return Pipeline(steps=numerical_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def pipeline_for(\n","    model: RegressorMixin,\n","    one_hot_encode: bool = True,\n","    one_hot_encode_scale: bool = True,\n","    numerical_scale: bool = True,\n","    scaler: TransformerMixin = RobustScaler(),\n","    pca: bool = False\n",") -> Pipeline:\n","    \"\"\" Creates a pipeline to use the data with a variety of models.\n","    \n","        Parameters:\n","            d: the pandas dataframe to use\n","            model: prediction model to add at the end of the pipeline\n","            one_hot_encode: whether to 1-hot encode categorical features\n","            one_hot_encode_scaler: whether 0/1 1-hot encoded features will also be scaled\n","            numerical_scale: whether to scale numerical features\n","            scaler: scaler object to use for scaling columns\n","            pca: whether to reduce the number of features via Principal Component Analysis before calling the model\n","            \n","        Returns:\n","            An sklearn pipeline. It can be trained directly or, if using PCA, passed to a cross-validation\n","            method to tune the number of components. The corresponding parameter to use in the param grid\n","            is 'pca__n_components'. Model hyperparameters to use in the param grid will be 'model__<...>'.\n","    \"\"\"\n","    \n","    transformers = [\n","        ('categorical',\n","         categorical_transformations(one_hot_encode, one_hot_encode_scale, scaler),\n","         categorical_features),\n","        ('numerical',\n","         numerical_transformations(numerical_scale, scaler),\n","         numerical_features)\n","    ]\n","        \n","    preprocessor = ColumnTransformer(transformers=transformers)\n","    \n","    # After preprocessing the input feature, one might want to try to\n","    # reduce their number using Principal Component Analysis. In this\n","    # case, one should first append a PCA object to the Preprocessor.\n","    # Otherwise, directly append the model.\n","    \n","    steps = [('preprocessing', preprocessor)]\n","    \n","    if pca:\n","        steps.append(('pca', PCA()))\n","        \n","    steps.append(('model', model))\n","    \n","    return Pipeline(steps=steps)"]},{"cell_type":"markdown","metadata":{},"source":["## Example 1: Multiple Linear Regression\n","\n","In this example I use:\n","* A transformation pipeline with imputing and 1-hot encoding of categorical column, imputing of numerical columns, and scaling of all columns (using a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n","* Principal component analysis using [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). We choose the best number of components via cross-validation (hence the use of `GridSearchCV`), trying values $\\{15, 15, 20, 25, \\ldots, 80\\}$.\n","* A linear regression model, using [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n","* Target transformation, using [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) with a target normal distribution and using 900 quantiles."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["linear_model = pipeline_for(\n","    model=TransformedTargetRegressor(\n","        regressor=LinearRegression(),\n","        transformer=QuantileTransformer(n_quantiles=900, output_distribution='normal')\n","    ),\n","    scaler=StandardScaler(),\n","    pca=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["I now use 5-fold cross-validation to select the best PCA hyperparameter (the number of principal components).\n","I validate using the mean squared log error, because this is the intrelevant if we want to participate in the [description of the Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["linear_model_gs = GridSearchCV(\n","    estimator=linear_model,\n","    param_grid={\n","        'pca__n_components': range(15, 85, 5)\n","    },\n","    scoring='neg_mean_squared_log_error',\n","    n_jobs=4,\n","    cv=5\n",")\n","linear_model_gs.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["linear_model_gs.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["#### Estimating the quality of the model using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = linear_model_gs.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Example 2: Multiple Linear Regression with LASSO Regularisation\n","\n","This model will look similar to the previous one, except that:\n","1. I apply a Lasso regularsation term and, therefore, I must tune its hyperparameter $\\alpha$.\n","2. Because Lasso performs implicit feature selection, I do not use PCA.\n","\n","The grid search to find the best value of $\\alpha$ uses a log-space with 20 entries between $10^{-6}$ and $10^1$.\n","I use 5-fold CV to tune this hyperparameter."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# I use max_iter=10000 because Lasso's default 1000 are usually\n","# not enought to ensure convergence.\n","lasso = pipeline_for(\n","    model=TransformedTargetRegressor(\n","        regressor=Lasso(max_iter=10000),\n","        transformer=QuantileTransformer(n_quantiles=900, output_distribution='normal')\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lasso_gs = GridSearchCV(\n","    estimator=lasso,\n","    param_grid={\n","        'model__regressor__alpha': np.logspace(start=-6, stop=1, num=20)\n","    },\n","    scoring='neg_mean_squared_log_error',\n","    n_jobs=4,\n","    cv=5\n",")\n","lasso_gs.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lasso_gs.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["#### Estimating the quality of the model using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = lasso_gs.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Example 3: Ada Boost\n","\n","Here I will be using an Ada(ptive)Boost method, with a simple decision tree regressor as the base estimator.\n","Sklearn's [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) has quite a number of hyperparameter.\n","Here I only tune one (`max_depth`); moreover, I tune two hyperparameters of `AdaBoostRegressor`, namely `n_estimators` and `learning_rate`.\n","\n","I will 1-hot encode the categorical features, because `DecisionTreeRegressor` is not able to deal with categorical features natively.\n","I will not scale the features, as I don't see much point in doing so with a tree-based estimator (it amounts to just a rescaling of the splitting thresholds)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ada_boost = pipeline_for(\n","    model=AdaBoostRegressor(base_estimator=DecisionTreeRegressor()),\n","    one_hot_encode_scale=False,\n","    numerical_scale=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ada_boost_gs = GridSearchCV(\n","    estimator=ada_boost,\n","    param_grid={\n","        'model__n_estimators': [10, 50, 100, 500, 1000],\n","        'model__learning_rate': [0.001, 0.01, 0.1, 1],\n","        'model__base_estimator__max_depth': range(5, 11)\n","    },\n","    scoring='neg_mean_squared_log_error',\n","    n_jobs=4,\n","    cv=5\n",")\n","ada_boost_gs.fit(X_train, y_train.values.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ada_boost_gs.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["#### Estimating the quality of the model using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = ada_boost_gs.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Example 4: XGBoost\n","\n","Next, trying an [`XGBRegressor`](https://xgboost.readthedocs.io/en/stable/python/python_api.html).\n","Note how this model also doesn't (yet) support categorical features and so I must 1-hot encode them."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb = pipeline_for(\n","    model=XGBRegressor(eval_metric=mean_squared_log_error),\n","    one_hot_encode=True,\n","    one_hot_encode_scale=False,\n","    numerical_scale=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb_cv = GridSearchCV(\n","    estimator=xgb,\n","    param_grid={\n","        'model__n_estimators': [10, 50, 100, 500, 1000],\n","        'model__learning_rate': [0.001, 0.01, 0.1, 1],\n","        'model__max_depth': range(5, 11)\n","    },\n","    scoring='neg_mean_squared_log_error',\n","    n_jobs=1,\n","    cv=5,\n","    verbose=4\n",")\n","xgb_cv.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb_cv.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["#### Estimating the quality of the model using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = xgb_cv.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=np.maximum(predictions, 0))"]},{"cell_type":"markdown","metadata":{},"source":["## Example 5: CatBoost\n","\n","Finally, we use [`CatBoostRegressor`](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor).\n","Because this model supports categorical features, we turn off 1-hot encoding."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["catb = pipeline_for(\n","    model=CatBoostRegressor(cat_features=list(range(len(categorical_features)))),\n","    one_hot_encode=False,\n","    one_hot_encode_scale=False,\n","    numerical_scale=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["catb.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = catb.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Building a stacked regressor with all the models above\n","\n","I now want to build a stacked regressor, combining all the models I tried until now and using `CatBoostRegressor` as the meta-estimator.\n","This is no trivial task, because each regressor requires different preprocessing steps which, ultimately, feed the estimator with different sets of features.\n","It will use the best hyperparameters tuned above, so I will not perform any hyperparameter tuning on the stacked regressor.\n","\n","The main pipeline to use with [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html) will be the following:\n","1. Perform data imputation, as this is a step required by all models.\n","2. `SimpleImputer` transforms the pandas dataframe it receive in input into a numpy array.\n","This is not convenient, because later preprocessing steps used by the models rely on the data to be in `DataFrame` format (for example, because they refer to the columns by name).\n","Therefore, I add a `back_to_pandas` transformation which brings the numpy values back into `DataFrame` form.\n","3. Finally, the model itself `StackingRegressor` with all its estimators and the meta-estimator.\n","Each of the estimators will take care of those preprocessing steps which are peculiar to its model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def back_to_pandas(values, imputer) -> pd.DataFrame:\n","    \"\"\" Transform raw X values back into a pandas DataFrame.\n","    \n","        Parameters:\n","            values: the X values as a numpy 2d array\n","            imputer: the imputer object which generated the X values and retains the column names\n","            \n","        Returns:\n","            A pandas DataFrame with the data (including the imputed values) and proper column names\n","    \"\"\"\n","    \n","    df = pd.DataFrame(values, columns=imputer.get_feature_names_out())\n","    for column in df.columns:\n","        if 'numerical' in column:\n","            df[column] = df[column].astype(np.float64)\n","        elif 'categorical' in column:\n","            df[column] = pd.Categorical(df[column])\n","    return df\n","\n","class CatBoostRegressorAutoDetectCategorical(CatBoostRegressor):\n","    \"\"\" A modified version of CatBoostRegressor which automatically detects categorical features.\n","        If the fit() method is called with a pandas DataFrame, it detects using the column dtype.\n","        Otherwise, it attempts to convert values to float. It marks columns in which it fails as\n","        categorical columns.\n","    \"\"\"\n","    \n","    def fit(self, X, y=None, **fit_params):\n","        if hasattr(X, 'columns'):\n","            cat = [column for column in X.columns if X[column].dtype == 'category']\n","            return super().fit(X, y, cat_features=cat, **fit_params)\n","        \n","        cat_cols = set()\n","        for row in X:\n","            for col, val in enumerate(row):\n","                try:\n","                    float(val)\n","                except ValueError:\n","                    cat_cols.add(col)\n","        \n","        return super().fit(X, y, cat_features=list(sorted(cat_cols)), **fit_params)\n","\n","### Preprocessing: Imputation\n","# Impute a constant 'None' for missing categorical values.\n","# Impute the median for missing numerical values.\n","sr_imputer = ColumnTransformer(transformers=[\n","    ('sr_imputer_categorical',\n","        SimpleImputer(strategy='constant', fill_value='None'),\n","        make_column_selector(dtype_include='category')),\n","    ('sr_imputer_numerical',\n","        SimpleImputer(strategy='median'),\n","        make_column_selector(dtype_include=np.number))\n","], verbose=True)\n","\n","### Preprocessing: 1-hot encoding\n","# Encode categorical columns.\n","# Skip numerical columns.\n","sr_encoder = ColumnTransformer(transformers=[\n","    ('sr_econder_categorical',\n","        OneHotEncoder(sparse=False, handle_unknown='ignore'),\n","        make_column_selector(dtype_include='category')),\n","    ('sr_encoder_numerical',\n","        'passthrough',\n","        make_column_selector(dtype_exclude='category'))\n","], verbose=True)\n","\n","### Preprocessing back-to-pandas\n","# Get back a DataFrame after imputation\n","sr_back_to_pandas = FunctionTransformer(\n","    func=lambda values: back_to_pandas(values, sr_imputer)\n",")\n","\n","### Linear model (with PCA and target transformation)\n","sr_linear_inner = TransformedTargetRegressor(\n","    regressor=make_pipeline(PCA(n_components=50), LinearRegression()),\n","    transformer=QuantileTransformer(n_quantiles=900, output_distribution='normal')\n",")\n","\n","### Pipeline for the linear model (with preprocessing)\n","sr_linear = Pipeline(steps=[\n","    ('sr_linear_one_hot_encoding', sr_encoder),\n","    ('sr_linear_scaling', StandardScaler()),\n","    ('sr_linear_model', sr_linear_inner)\n","], verbose=True)\n","\n","### Lasso model (with target transformation)\n","sr_lasso_inner = TransformedTargetRegressor(\n","    regressor=Lasso(alpha=0.0008858667904100823, max_iter=10000),\n","    transformer=QuantileTransformer(n_quantiles=900, output_distribution='normal')\n",")\n","\n","### Pipeline for the lasso model (with preprocessing)\n","sr_lasso = Pipeline(steps=[\n","    ('sr_lasso_one_hot_encoding', sr_encoder),\n","    ('sr_lasso_scaling', RobustScaler()),\n","    ('sr_lasso_model', sr_lasso_inner)\n","], verbose=True)\n","\n","### Adaboost model\n","sr_adaboost_inner = AdaBoostRegressor(\n","    base_estimator=DecisionTreeRegressor(max_depth=10),\n","    learning_rate=1,\n","    n_estimators=1000\n",")\n","\n","### Pipeline for the Adaboost model (with preprocessing)\n","sr_adaboost = Pipeline(steps=[\n","    ('sr_adaboost_one_hot_encoding', sr_encoder),\n","    ('sr_adaboost_model', sr_adaboost_inner)\n","], verbose=True)\n","\n","\n","### XGBoost model\n","sr_xgboost_inner = XGBRegressor(\n","    n_estimators=1000,\n","    learning_rate=0.01,\n","    max_depth=5)\n","\n","### Pipeline for the XGBoost model (with preprocessing)\n","sr_xgboost = Pipeline(steps=[\n","    ('sr_xgboost_one_hot_encoding', sr_encoder),\n","    ('sr_xgboost_model', sr_xgboost_inner)\n","], verbose=True)\n","\n","### Catboost model (does not need any model-specifing preprocessing)\n","sr_catboost = CatBoostRegressorAutoDetectCategorical()\n","\n","### Stacking regressor model\n","sr_stacking_regressor = StackingRegressor(\n","    estimators=[\n","        ('sr_stacking_regressor_linear_regression', sr_linear),\n","        ('sr_stacking_regressor_lasso_regression', sr_lasso),\n","        ('sr_stacking_regressor_ada_boost', sr_adaboost),\n","        ('sr_stacking_regressor_xgboost', sr_xgboost),\n","        ('sr_stacking_regressor_catboost', sr_catboost)\n","    ],\n","    final_estimator=CatBoostRegressorAutoDetectCategorical(),\n","    passthrough=True,\n","    verbose=3\n",")\n","\n","### Pipeline for the stacking regressor model (with preprocessing, i.e., data imputation)\n","stacking_regressor = Pipeline(steps=[\n","    ('stacking_regressor_imputation', sr_imputer),\n","    ('stacking_regressor_back_to_pandas', sr_back_to_pandas),\n","    ('stacking_regressor_model', sr_stacking_regressor)\n","], verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stacking_regressor.fit(X_train, y_train.values.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = stacking_regressor.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean_squared_log_error(y_true=y_test, y_pred=predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Retraining the winning model on the entire dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stacking_regressor.fit(X, y)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
